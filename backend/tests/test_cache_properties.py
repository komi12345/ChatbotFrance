"""
Property-based tests for CacheService.

Tests the correctness properties defined in the design document for the
Performance Optimization feature.
"""
import pytest
from hypothesis import given, settings as hyp_settings, strategies as st, HealthCheck
from unittest.mock import MagicMock, patch

# Configure Hypothesis for CI: minimum 100 iterations
hyp_settings.register_profile(
    "ci",
    max_examples=100,
    suppress_health_check=[HealthCheck.too_slow],
    deadline=None
)
hyp_settings.load_profile("ci")


# ==========================================================================
# STRATEGIES FOR CACHE SERVICE TESTS
# ==========================================================================

# Strategy for namespace strings (alphanumeric with underscores)
namespace_strategy = st.text(
    alphabet="abcdefghijklmnopqrstuvwxyz0123456789_",
    min_size=1,
    max_size=30
).filter(lambda x: len(x.strip()) > 0)

# Strategy for key strings (alphanumeric with underscores and colons)
key_strategy = st.text(
    alphabet="abcdefghijklmnopqrstuvwxyz0123456789_:",
    min_size=1,
    max_size=50
).filter(lambda x: len(x.strip()) > 0)

# Strategy for protected prefixes
protected_prefix_strategy = st.sampled_from(["whatsapp:", "campaign:", "celery"])


class TestCacheKeyIsolationProperty:
    """
    Property 1: Cache Key Isolation
    
    *For any* key generated by CacheService, the key SHALL start with the prefix
    "cache:" and SHALL NOT start with any protected prefix ("whatsapp:", "campaign:",
    "celery").
    
    **Feature: performance-optimization, Property 1: Cache Key Isolation**
    **Validates: Requirements 3.1, 3.5**
    """

    @given(namespace=namespace_strategy, key=key_strategy)
    def test_generated_key_starts_with_cache_prefix(
        self,
        namespace: str,
        key: str
    ):
        """
        **Feature: performance-optimization, Property 1: Cache Key Isolation**
        **Validates: Requirements 3.1, 3.5**
        
        For any namespace and key combination, the generated cache key should
        always start with the "cache:" prefix.
        """
        from app.services.cache_service import CacheService
        
        service = CacheService.__new__(CacheService)
        service.CACHE_PREFIX = "cache:"
        
        cache_key = service._make_key(namespace, key)
        
        # Property: key starts with "cache:"
        assert cache_key.startswith("cache:"), \
            f"Expected key to start with 'cache:', got '{cache_key}'"

    @given(namespace=namespace_strategy, key=key_strategy)
    def test_generated_key_never_starts_with_protected_prefix(
        self,
        namespace: str,
        key: str
    ):
        """
        **Feature: performance-optimization, Property 1: Cache Key Isolation**
        **Validates: Requirements 3.1, 3.5**
        
        For any namespace and key combination, the generated cache key should
        never start with any protected prefix.
        """
        from app.services.cache_service import CacheService
        
        service = CacheService.__new__(CacheService)
        service.CACHE_PREFIX = "cache:"
        service.PROTECTED_PREFIXES = ("whatsapp:", "campaign:", "celery")
        
        cache_key = service._make_key(namespace, key)
        
        # Property: key does not start with any protected prefix
        for protected in service.PROTECTED_PREFIXES:
            assert not cache_key.startswith(protected), \
                f"Key '{cache_key}' should not start with protected prefix '{protected}'"

    @given(namespace=namespace_strategy, key=key_strategy)
    def test_generated_key_is_not_protected(
        self,
        namespace: str,
        key: str
    ):
        """
        **Feature: performance-optimization, Property 1: Cache Key Isolation**
        **Validates: Requirements 3.1, 3.5**
        
        For any namespace and key combination, the generated cache key should
        not be detected as a protected key.
        """
        from app.services.cache_service import CacheService
        
        service = CacheService.__new__(CacheService)
        service.CACHE_PREFIX = "cache:"
        service.PROTECTED_PREFIXES = ("whatsapp:", "campaign:", "celery")
        
        cache_key = service._make_key(namespace, key)
        
        # Property: generated key is not protected
        assert not service._is_protected_key(cache_key), \
            f"Generated key '{cache_key}' should not be detected as protected"

    @given(namespace=namespace_strategy, key=key_strategy)
    def test_key_format_is_consistent(
        self,
        namespace: str,
        key: str
    ):
        """
        **Feature: performance-optimization, Property 1: Cache Key Isolation**
        **Validates: Requirements 3.1**
        
        For any namespace and key combination, the generated cache key should
        follow the format "cache:{namespace}:{key}".
        """
        from app.services.cache_service import CacheService
        
        service = CacheService.__new__(CacheService)
        service.CACHE_PREFIX = "cache:"
        
        cache_key = service._make_key(namespace, key)
        
        # Property: key follows expected format
        expected = f"cache:{namespace}:{key}"
        assert cache_key == expected, \
            f"Expected '{expected}', got '{cache_key}'"

    @given(protected_prefix=protected_prefix_strategy)
    def test_protected_keys_are_detected(
        self,
        protected_prefix: str
    ):
        """
        **Feature: performance-optimization, Property 1: Cache Key Isolation**
        **Validates: Requirements 3.5**
        
        For any protected prefix, keys starting with that prefix should be
        detected as protected.
        """
        from app.services.cache_service import CacheService
        
        service = CacheService.__new__(CacheService)
        service.PROTECTED_PREFIXES = ("whatsapp:", "campaign:", "celery")
        
        # Create a key with the protected prefix
        test_key = f"{protected_prefix}some_key"
        
        # Property: protected keys are detected
        assert service._is_protected_key(test_key), \
            f"Key '{test_key}' should be detected as protected"

    @given(
        namespace=namespace_strategy,
        key=key_strategy,
        protected_prefix=protected_prefix_strategy
    )
    def test_set_operation_never_writes_to_protected_keys(
        self,
        namespace: str,
        key: str,
        protected_prefix: str
    ):
        """
        **Feature: performance-optimization, Property 1: Cache Key Isolation**
        **Validates: Requirements 3.5**
        
        For any set operation, if the resulting key would be protected,
        the operation should fail and not write to Redis.
        """
        from app.services.cache_service import CacheService
        from datetime import timedelta
        
        service = CacheService.__new__(CacheService)
        service.CACHE_PREFIX = "cache:"
        service.PROTECTED_PREFIXES = ("whatsapp:", "campaign:", "celery")
        service.DEFAULT_TTL = timedelta(seconds=60)
        
        # Track if setex was called
        setex_called = False
        
        def mock_setex(*args, **kwargs):
            nonlocal setex_called
            setex_called = True
        
        mock_redis = MagicMock()
        mock_redis.setex = mock_setex
        service._redis = mock_redis
        
        # Override _make_key to return a protected key
        service._make_key = lambda ns, k: f"{protected_prefix}test_key"
        
        # Attempt to set
        result = service.set(namespace, key, {"value": 1})
        
        # Property: set should fail for protected keys
        assert result is False, \
            f"Set should return False for protected key with prefix '{protected_prefix}'"
        
        # Property: Redis setex should not be called
        assert not setex_called, \
            f"Redis setex should not be called for protected key with prefix '{protected_prefix}'"


# ==========================================================================
# STRATEGIES FOR ROUND-TRIP TESTS
# ==========================================================================

# Strategy for JSON-serializable primitive values
json_primitive_strategy = st.one_of(
    st.none(),
    st.booleans(),
    st.integers(min_value=-2**31, max_value=2**31 - 1),
    st.floats(allow_nan=False, allow_infinity=False),
    st.text(min_size=0, max_size=100)
)

# Strategy for JSON-serializable dictionaries (simple)
json_dict_strategy = st.dictionaries(
    keys=st.text(
        alphabet="abcdefghijklmnopqrstuvwxyz_",
        min_size=1,
        max_size=20
    ),
    values=json_primitive_strategy,
    min_size=0,
    max_size=10
)

# Strategy for JSON-serializable lists
json_list_strategy = st.lists(
    json_primitive_strategy,
    min_size=0,
    max_size=10
)

# Combined strategy for any JSON-serializable value
json_value_strategy = st.one_of(
    json_primitive_strategy,
    json_dict_strategy,
    json_list_strategy
)


class TestCacheRoundTripProperty:
    """
    Property 2: Cache Round-Trip Consistency
    
    *For any* valid data object, storing it in the cache and then retrieving it
    SHALL return an equivalent object.
    
    **Feature: performance-optimization, Property 2: Cache Round-Trip Consistency**
    **Validates: Requirements 3.2**
    """

    @given(
        namespace=namespace_strategy,
        key=key_strategy,
        value=json_dict_strategy
    )
    def test_round_trip_preserves_dict_data(
        self,
        namespace: str,
        key: str,
        value: dict
    ):
        """
        **Feature: performance-optimization, Property 2: Cache Round-Trip Consistency**
        **Validates: Requirements 3.2**
        
        For any namespace, key, and dictionary value, storing and retrieving
        should return an equivalent dictionary.
        """
        from app.services.cache_service import CacheService
        from datetime import timedelta
        import json
        
        service = CacheService.__new__(CacheService)
        service.CACHE_PREFIX = "cache:"
        service.PROTECTED_PREFIXES = ("whatsapp:", "campaign:", "celery")
        service.DEFAULT_TTL = timedelta(seconds=60)
        service._hits = 0
        service._misses = 0
        
        # Simulate Redis storage with in-memory dict
        storage = {}
        
        def mock_setex(cache_key, ttl, serialized_value):
            storage[cache_key] = serialized_value
        
        def mock_get(cache_key):
            return storage.get(cache_key)
        
        mock_redis = MagicMock()
        mock_redis.setex = mock_setex
        mock_redis.get = mock_get
        service._redis = mock_redis
        
        # Set the value
        set_result = service.set(namespace, key, value)
        assert set_result is True, "Set operation should succeed"
        
        # Get the value back
        retrieved = service.get(namespace, key)
        
        # Property: round-trip should preserve the data
        assert retrieved == value, \
            f"Round-trip failed: stored {value}, retrieved {retrieved}"

    @given(
        namespace=namespace_strategy,
        key=key_strategy,
        value=json_list_strategy
    )
    def test_round_trip_preserves_list_data(
        self,
        namespace: str,
        key: str,
        value: list
    ):
        """
        **Feature: performance-optimization, Property 2: Cache Round-Trip Consistency**
        **Validates: Requirements 3.2**
        
        For any namespace, key, and list value, storing and retrieving
        should return an equivalent list.
        """
        from app.services.cache_service import CacheService
        from datetime import timedelta
        
        service = CacheService.__new__(CacheService)
        service.CACHE_PREFIX = "cache:"
        service.PROTECTED_PREFIXES = ("whatsapp:", "campaign:", "celery")
        service.DEFAULT_TTL = timedelta(seconds=60)
        service._hits = 0
        service._misses = 0
        
        # Simulate Redis storage with in-memory dict
        storage = {}
        
        def mock_setex(cache_key, ttl, serialized_value):
            storage[cache_key] = serialized_value
        
        def mock_get(cache_key):
            return storage.get(cache_key)
        
        mock_redis = MagicMock()
        mock_redis.setex = mock_setex
        mock_redis.get = mock_get
        service._redis = mock_redis
        
        # Set the value
        set_result = service.set(namespace, key, value)
        assert set_result is True, "Set operation should succeed"
        
        # Get the value back
        retrieved = service.get(namespace, key)
        
        # Property: round-trip should preserve the data
        assert retrieved == value, \
            f"Round-trip failed: stored {value}, retrieved {retrieved}"

    @given(
        namespace=namespace_strategy,
        key=key_strategy,
        value=json_primitive_strategy
    )
    def test_round_trip_preserves_primitive_data(
        self,
        namespace: str,
        key: str,
        value
    ):
        """
        **Feature: performance-optimization, Property 2: Cache Round-Trip Consistency**
        **Validates: Requirements 3.2**
        
        For any namespace, key, and primitive value (int, float, str, bool, None),
        storing and retrieving should return an equivalent value.
        """
        from app.services.cache_service import CacheService
        from datetime import timedelta
        
        service = CacheService.__new__(CacheService)
        service.CACHE_PREFIX = "cache:"
        service.PROTECTED_PREFIXES = ("whatsapp:", "campaign:", "celery")
        service.DEFAULT_TTL = timedelta(seconds=60)
        service._hits = 0
        service._misses = 0
        
        # Simulate Redis storage with in-memory dict
        storage = {}
        
        def mock_setex(cache_key, ttl, serialized_value):
            storage[cache_key] = serialized_value
        
        def mock_get(cache_key):
            return storage.get(cache_key)
        
        mock_redis = MagicMock()
        mock_redis.setex = mock_setex
        mock_redis.get = mock_get
        service._redis = mock_redis
        
        # Set the value
        set_result = service.set(namespace, key, value)
        assert set_result is True, "Set operation should succeed"
        
        # Get the value back
        retrieved = service.get(namespace, key)
        
        # Property: round-trip should preserve the data
        assert retrieved == value, \
            f"Round-trip failed: stored {value}, retrieved {retrieved}"

    @given(
        namespace=namespace_strategy,
        key=key_strategy,
        total_contacts=st.integers(min_value=0, max_value=1000000),
        total_messages=st.integers(min_value=0, max_value=10000000),
        total_campaigns=st.integers(min_value=0, max_value=10000)
    )
    def test_round_trip_preserves_stats_like_data(
        self,
        namespace: str,
        key: str,
        total_contacts: int,
        total_messages: int,
        total_campaigns: int
    ):
        """
        **Feature: performance-optimization, Property 2: Cache Round-Trip Consistency**
        **Validates: Requirements 3.2**
        
        For any stats-like data structure (similar to what /messages/stats returns),
        storing and retrieving should return an equivalent object.
        """
        from app.services.cache_service import CacheService
        from datetime import timedelta
        
        service = CacheService.__new__(CacheService)
        service.CACHE_PREFIX = "cache:"
        service.PROTECTED_PREFIXES = ("whatsapp:", "campaign:", "celery")
        service.DEFAULT_TTL = timedelta(seconds=60)
        service._hits = 0
        service._misses = 0
        
        # Simulate Redis storage with in-memory dict
        storage = {}
        
        def mock_setex(cache_key, ttl, serialized_value):
            storage[cache_key] = serialized_value
        
        def mock_get(cache_key):
            return storage.get(cache_key)
        
        mock_redis = MagicMock()
        mock_redis.setex = mock_setex
        mock_redis.get = mock_get
        service._redis = mock_redis
        
        # Create stats-like data
        stats_data = {
            "total_contacts": total_contacts,
            "total_messages": total_messages,
            "total_campaigns": total_campaigns,
            "messages_sent": total_messages // 2,
            "messages_delivered": total_messages // 3,
            "messages_read": total_messages // 4
        }
        
        # Set the value
        set_result = service.set(namespace, key, stats_data)
        assert set_result is True, "Set operation should succeed"
        
        # Get the value back
        retrieved = service.get(namespace, key)
        
        # Property: round-trip should preserve the data
        assert retrieved == stats_data, \
            f"Round-trip failed: stored {stats_data}, retrieved {retrieved}"

    @given(
        namespace=namespace_strategy,
        key=key_strategy
    )
    def test_round_trip_with_nested_structure(
        self,
        namespace: str,
        key: str
    ):
        """
        **Feature: performance-optimization, Property 2: Cache Round-Trip Consistency**
        **Validates: Requirements 3.2**
        
        For nested data structures (dict containing lists and other dicts),
        storing and retrieving should return an equivalent object.
        """
        from app.services.cache_service import CacheService
        from datetime import timedelta
        
        service = CacheService.__new__(CacheService)
        service.CACHE_PREFIX = "cache:"
        service.PROTECTED_PREFIXES = ("whatsapp:", "campaign:", "celery")
        service.DEFAULT_TTL = timedelta(seconds=60)
        service._hits = 0
        service._misses = 0
        
        # Simulate Redis storage with in-memory dict
        storage = {}
        
        def mock_setex(cache_key, ttl, serialized_value):
            storage[cache_key] = serialized_value
        
        def mock_get(cache_key):
            return storage.get(cache_key)
        
        mock_redis = MagicMock()
        mock_redis.setex = mock_setex
        mock_redis.get = mock_get
        service._redis = mock_redis
        
        # Create nested data structure (like category with contacts)
        nested_data = {
            "id": 1,
            "name": "Test Category",
            "contacts": [
                {"id": 1, "phone": "+22912345678", "name": "Contact 1"},
                {"id": 2, "phone": "+22987654321", "name": "Contact 2"}
            ],
            "metadata": {
                "created_at": "2024-12-28T10:00:00Z",
                "updated_at": "2024-12-28T12:00:00Z",
                "tags": ["important", "active"]
            }
        }
        
        # Set the value
        set_result = service.set(namespace, key, nested_data)
        assert set_result is True, "Set operation should succeed"
        
        # Get the value back
        retrieved = service.get(namespace, key)
        
        # Property: round-trip should preserve the data
        assert retrieved == nested_data, \
            f"Round-trip failed: stored {nested_data}, retrieved {retrieved}"


# ==========================================================================
# PROPERTY 6: CACHE MISS RATE WARNING
# ==========================================================================


class TestCacheMissRateWarningProperty:
    """
    Property 6: Cache Miss Rate Warning
    
    *For any* cache metrics where miss rate exceeds 50%, the system SHALL log a warning.
    
    **Feature: performance-optimization, Property 6: Cache Miss Rate Warning**
    **Validates: Requirements 6.3**
    """

    @given(
        hits=st.integers(min_value=0, max_value=10000),
        misses=st.integers(min_value=0, max_value=10000)
    )
    def test_warning_logged_when_miss_rate_exceeds_50_percent(
        self,
        hits: int,
        misses: int
    ):
        """
        **Feature: performance-optimization, Property 6: Cache Miss Rate Warning**
        **Validates: Requirements 6.3**
        
        For any combination of hits and misses where miss rate > 50% and total > 10,
        the system should log a warning when get_metrics() is called.
        """
        from app.services.cache_service import CacheService
        import logging
        
        service = CacheService.__new__(CacheService)
        service._hits = hits
        service._misses = misses
        
        total = hits + misses
        hit_rate = (hits / total * 100) if total > 0 else 0.0
        miss_rate = 100 - hit_rate
        
        # Capture log output
        with patch('app.services.cache_service.logger') as mock_logger:
            metrics = service.get_metrics()
            
            # Property: if total > 10 and hit_rate < 50, warning should be logged
            if total > 10 and hit_rate < 50:
                mock_logger.warning.assert_called()
                # Verify the warning message contains relevant info
                call_args = mock_logger.warning.call_args[0][0]
                assert "hit rate" in call_args.lower() or "low" in call_args.lower(), \
                    f"Warning message should mention hit rate, got: {call_args}"
            else:
                # No warning should be logged if conditions not met
                mock_logger.warning.assert_not_called()

    @given(
        misses=st.integers(min_value=11, max_value=10000)
    )
    def test_warning_always_logged_when_all_misses(
        self,
        misses: int
    ):
        """
        **Feature: performance-optimization, Property 6: Cache Miss Rate Warning**
        **Validates: Requirements 6.3**
        
        For any scenario with only misses (0 hits) and total > 10,
        the system should always log a warning (100% miss rate).
        """
        from app.services.cache_service import CacheService
        
        service = CacheService.__new__(CacheService)
        service._hits = 0
        service._misses = misses
        
        with patch('app.services.cache_service.logger') as mock_logger:
            metrics = service.get_metrics()
            
            # Property: 100% miss rate should always trigger warning
            mock_logger.warning.assert_called_once()
            
            # Verify metrics are correct
            assert metrics["hits"] == 0
            assert metrics["misses"] == misses
            assert metrics["hit_rate"] == 0.0

    @given(
        hits=st.integers(min_value=6, max_value=10000)
    )
    def test_no_warning_when_hit_rate_above_50_percent(
        self,
        hits: int
    ):
        """
        **Feature: performance-optimization, Property 6: Cache Miss Rate Warning**
        **Validates: Requirements 6.3**
        
        For any scenario where hit rate >= 50% (hits >= misses),
        no warning should be logged.
        """
        from app.services.cache_service import CacheService
        
        service = CacheService.__new__(CacheService)
        # Set misses to be less than or equal to hits (hit_rate >= 50%)
        misses = hits - 1 if hits > 0 else 0
        service._hits = hits
        service._misses = misses
        
        with patch('app.services.cache_service.logger') as mock_logger:
            metrics = service.get_metrics()
            
            # Property: hit_rate >= 50% should not trigger warning
            mock_logger.warning.assert_not_called()
            
            # Verify hit_rate is indeed >= 50%
            assert metrics["hit_rate"] >= 50.0

    @given(
        hits=st.integers(min_value=0, max_value=5),
        misses=st.integers(min_value=0, max_value=5)
    )
    def test_no_warning_when_total_requests_low(
        self,
        hits: int,
        misses: int
    ):
        """
        **Feature: performance-optimization, Property 6: Cache Miss Rate Warning**
        **Validates: Requirements 6.3**
        
        For any scenario where total requests <= 10,
        no warning should be logged (not enough data to determine pattern).
        """
        from app.services.cache_service import CacheService
        
        service = CacheService.__new__(CacheService)
        service._hits = hits
        service._misses = misses
        
        total = hits + misses
        
        with patch('app.services.cache_service.logger') as mock_logger:
            metrics = service.get_metrics()
            
            # Property: total <= 10 should not trigger warning
            if total <= 10:
                mock_logger.warning.assert_not_called()

    @given(
        hits=st.integers(min_value=0, max_value=10000),
        misses=st.integers(min_value=0, max_value=10000)
    )
    def test_metrics_calculation_is_correct(
        self,
        hits: int,
        misses: int
    ):
        """
        **Feature: performance-optimization, Property 6: Cache Miss Rate Warning**
        **Validates: Requirements 6.1, 6.3**
        
        For any combination of hits and misses, the metrics calculation
        should be mathematically correct.
        """
        from app.services.cache_service import CacheService
        
        service = CacheService.__new__(CacheService)
        service._hits = hits
        service._misses = misses
        
        with patch('app.services.cache_service.logger'):
            metrics = service.get_metrics()
        
        # Property: metrics values are correct
        assert metrics["hits"] == hits, \
            f"Expected hits={hits}, got {metrics['hits']}"
        assert metrics["misses"] == misses, \
            f"Expected misses={misses}, got {metrics['misses']}"
        assert metrics["total"] == hits + misses, \
            f"Expected total={hits + misses}, got {metrics['total']}"
        
        # Property: hit_rate calculation is correct
        total = hits + misses
        expected_hit_rate = round((hits / total * 100), 2) if total > 0 else 0.0
        assert metrics["hit_rate"] == expected_hit_rate, \
            f"Expected hit_rate={expected_hit_rate}, got {metrics['hit_rate']}"

    @given(
        hits=st.integers(min_value=1, max_value=100),
        miss_multiplier=st.integers(min_value=2, max_value=10)
    )
    def test_warning_contains_useful_information(
        self,
        hits: int,
        miss_multiplier: int
    ):
        """
        **Feature: performance-optimization, Property 6: Cache Miss Rate Warning**
        **Validates: Requirements 6.3**
        
        For any scenario triggering a warning, the warning message should
        contain useful debugging information (hit rate, hits count, misses count).
        """
        from app.services.cache_service import CacheService
        
        service = CacheService.__new__(CacheService)
        # Ensure miss rate > 50% and total > 10
        misses = hits * miss_multiplier
        service._hits = hits
        service._misses = misses
        
        total = hits + misses
        
        # Only test if conditions for warning are met
        if total > 10:
            with patch('app.services.cache_service.logger') as mock_logger:
                metrics = service.get_metrics()
                
                # Property: warning should be called with informative message
                mock_logger.warning.assert_called_once()
                warning_message = mock_logger.warning.call_args[0][0]
                
                # The warning should contain the hit rate percentage
                assert "%" in warning_message or "hit" in warning_message.lower(), \
                    f"Warning should contain hit rate info, got: {warning_message}"
